% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backprop_namls.R
\name{backprop_namls}
\alias{backprop_namls}
\title{Backpropagation for NAMLS Model}
\usage{
backprop_namls(X, y, fwd, params, dropout_rate = 0)
}
\arguments{
\item{X}{Numeric matrix of input features (predictors), with columns as samples
and rows as features.}

\item{y}{Numeric vector of target values, same length as number of columns in \code{X}.}

\item{fwd}{List; result of a forward pass (\code{forward_namls()}), containing cached
activations, outputs, and intermediate values.}

\item{params}{List of model parameters (weights and biases) for all subnetworks.}

\item{dropout_rate}{Numeric (0â€“1); dropout rate applied in hidden layers. Default is 0.}
}
\value{
A list of gradients (\code{grads}) for all model parameters, including
weights, biases, and global coefficients (\code{beta_mu}, \code{beta_sigma}).
}
\description{
Computes gradients of the loss function with respect to all model parameters
in a NAMLS network using backpropagation. Supports dropout, ReLU activations,
and subnet-specific weight updates.
}
\examples{
\dontrun{
# Forward pass
fwd <- forward_namls(X, params)

# Backpropagation
grads <- backprop_namls(X, y, fwd, params, dropout_rate = 0.2)
}

}
