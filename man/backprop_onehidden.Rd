% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/backprop_onehidden.R
\name{backprop_onehidden}
\alias{backprop_onehidden}
\title{Backpropagation for a Single-Hidden-Layer Neural Network}
\usage{
backprop_onehidden(X, y, cache, params)
}
\arguments{
\item{X}{A numeric matrix of shape (p Ã— m), where p is the number of input features and m is the batch size.}

\item{y}{Numeric vector of length m containing the target values.}

\item{cache}{A list containing intermediate values from the forward pass:
\code{Z1}, \code{A1}, \code{Z2}, \code{mu}, \code{log_sigma}.}

\item{params}{A list of model parameters:
\code{W1}, \code{b1}, \code{W2}, \code{b2}.}
}
\value{
A list of gradients with respect to the parameters:
\itemize{
\item \code{dW1}: Gradient matrix for the first layer weights.
\item \code{db1}: Gradient vector for the first layer biases.
\item \code{dW2}: Gradient matrix for the second layer weights.
\item \code{db2}: Gradient vector for the second layer biases.
}
}
\description{
Computes the gradients of the parameters for a neural network with one hidden layer,
assuming a Gaussian likelihood with mean and log standard deviation outputs.
}
\details{
This function performs backpropagation using derivatives of the negative log-likelihood
under a normal distribution. The output layer returns both the predicted mean (\code{mu})
and log standard deviation (\code{log_sigma}). Gradients are averaged over the batch.
}
\examples{
# Example usage:
X <- matrix(rnorm(20), nrow = 4)
y <- rnorm(5)
params <- list(
  W1 = matrix(rnorm(8), nrow = 2),
  b1 = matrix(0, nrow = 2),
  W2 = matrix(rnorm(4), nrow = 2),
  b2 = matrix(0, nrow = 2)
)
cache <- forward_onehidden(X, params)
grads <- backprop_onehidden(X, y, cache, params)

}
