% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eval_DNN.R
\name{eval_DNN}
\alias{eval_DNN}
\title{Evaluate a Trained Neural Network Model}
\usage{
eval_DNN(object, split_output, verbose = TRUE)
}
\arguments{
\item{object}{Trained model object containing learned parameters (\code{params})
and target values (\code{targets}).}

\item{split_output}{List containing train/test split data. The element
\code{split_output$test} must contain the test set.}

\item{verbose}{Logical; if \code{TRUE} (default), prints evaluation metrics to
the console.}
}
\value{
A list with the following elements:
\item{fwd}{Forward pass results on the test set.}
\item{loss}{Negative log-likelihood loss on the test set.}
\item{mu}{Predicted means for the test samples.}
\item{sigma}{Predicted standard deviations for the test samples.}
\item{cover}{Proportion of true values within the 95 percent prediction interval.}
\item{rmse}{Root mean squared error on the test set.}
\item{mae}{Mean absolute error on the test set.}
\item{test_df_targets}{True target values for the test set.}
}
\description{
Evaluates a trained neural network on a given test set. Performs a forward
pass, calculates prediction uncertainty, loss, and key performance metrics.
Optionally prints evaluation results to the console.
}
\examples{
\dontrun{
  evaluation <- eval_DNN(model, split_output, verbose = TRUE)
}

}
