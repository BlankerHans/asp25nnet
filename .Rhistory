dataframe$unab
class(dataframe$unab)
class(as.dataframe(dataframe$unab))
dataframe["unab"]
class(dataframe["unab"])
split <- train_val_test(dataframe["unab"])
train <- split$train
val <- split$val
test <- split$test
nrow(dataframe) == nrow(test)+nrow(train)+nrow(val)
train_loader <- DataLoader(split$train)
targets <- dataframe$abh
dimensions <- getLayerDimensions(train_loader[[1]]$batch, 2, hidden_neurons = 3)
val
val_loader <- DataLoader(split$val)
val_loader
val_targets <- targets[val_loader$idx]
nrow(val_targets)
val_targets
val_loader
val_loader$idx
val
data <- abdom
load_all()
set.seed(42)
n <- 500
beta <- 2
sigma0 <- 0.5
# 1) Simuliere x
x <- abs(rnorm(n, mean = 0, sd = 1))
# 2) Berechne für jede x_i die Fehler‐Std-Dev
sigma_x <- sigma0 * x
# 3) Ziehe heteroskedastische Fehler
eps <- rnorm(n, mean = 0, sd = sigma_x)
# 4) Erzeuge y
y <- beta * x + eps
# Kurzer Blick auf Varianz in Abhängigkeit von x
plot(x, y,
xlab = "x",
ylab = "y",
main = "Heteroskedastie")
dataframe <- as.data.frame(cbind(x, y))
View(dataframe)
colnames(dataframe) <- c("unab", "abh")
split <- train_val_test(dataframe["unab"])
train <- split$train
val <- split$val
test <- split$test
nrow(dataframe) == nrow(test)+nrow(train)+nrow(val)
train_loader <- DataLoader(split$train)
targets <- dataframe$abh
val
rownames(val)
int(rownames(val))
integer(rownames(val))
apply(rownames(val), integer)
rownames(val)
traind_loader[[1]]$idx
train_loader[[1]]$idx
?apply
?rapply
?sapply
sapply(colnames(val), integer)
class(colnames(val))
class(as.vector(colnames(val)))
c(colnames(val))
colnames(val)
rownames(val)
class(rownames(val))
as.integer(rownames(val))
targets <- dataframe$abh
targets[as.integer(rownames(val))]
targets <- dataframe$abh
val_targets <- targets[as.integer(rownames(val))]
dimensions <- getLayerDimensions(train_loader[[1]]$batch, 2, hidden_neurons = 3)
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
train_loader[[1]]$batch
dimensions
dim(train_loader)
train_loader
train_loader[[1]]$batch
dim(train_loader[[1]]$batch)
dim(train_loader[[1]]$batch)[1]
source("~/Schreibtisch/Studium/ASP/asp25nnet/nnet25.R")
dimensions
train_loader[[1]]$batch
source("~/Schreibtisch/Studium/ASP/asp25nnet/testing.R")
source("~/Schreibtisch/Studium/ASP/asp25nnet/testing.R")
data <- abdom
load_all()
set.seed(42)
n <- 500
load_all()
set.seed(42)
n <- 500
beta <- 2
sigma0 <- 0.5
# 1) Simuliere x
x <- abs(rnorm(n, mean = 0, sd = 1))
# 2) Berechne für jede x_i die Fehler‐Std-Dev
sigma_x <- sigma0 * x
# 3) Ziehe heteroskedastische Fehler
eps <- rnorm(n, mean = 0, sd = sigma_x)
# 4) Erzeuge y
y <- beta * x + eps
# Kurzer Blick auf Varianz in Abhängigkeit von x
plot(x, y,
xlab = "x",
ylab = "y",
main = "Heteroskedastie")
dataframe <- as.data.frame(cbind(x, y))
View(dataframe)
colnames(dataframe) <- c("unab", "abh")
split <- train_val_test(dataframe["unab"])
train <- split$train
val <- split$val
test <- split$test
nrow(dataframe) == nrow(test)+nrow(train)+nrow(val)
train_loader <- DataLoader(split$train)
targets <- dataframe$abh
val_targets <- targets[as.integer(rownames(val))]
dimensions <- getLayerDimensions(train_loader[[1]]$batch, 2, hidden_neurons = 3)
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
train_network(train_loader,
targets,
dimensions)
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
dataframe
t(dataframe)
colnames(t(dataframe))
rownames(dataframe)
colnames(t(dataframe))
dataframe[["x"]]
dataframe["unab"]
class(dataframe["unab"])
class(train)
train
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
train_network(train_loader,
targets,
dimensions)
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
init_params(dimensions_list)
init_params(dimensions)
train_loader[[1]]$batch
dim(train_loader[[1]]$batch)
init_params(dimension)$W1
W1 <- init_params(dimensions)$W1
W1
dim(W1)
train_loader[[1]]$batch
dim(train_loader[[1]]$batch)
class(W1)
class(train_loader[[1]]$batch)
W1%*% train_loader[[1]]$batch
dim(W1%*% train_loader[[1]]$batch)
params <- init_params(dimensions)
W1 <- params$W1
params$W2
params$W2%*%(W1%*% train_loader[[1]]$batch)
dimensions
train_network_val_adam(train_loader, targets, dimensions, val, val_targets)
length(train_loader)
train_loader
val targets
val_targets
dim(val)
dim(W1)
clas(val)
class(val)
t(val)
dim(t(val))
W1%*%t(val)
train_network_val_adam(train_loader, targets, dimensions, t(val), val_targets)
load_all()
train(train_loader, targets, dimensions, t(val), val_targets)
train(train_loader, targets, dimensions, t(val), val_targets)
load_all()
data <- abdom
load_all()
set.seed(42)
n <- 500
beta <- 2
sigma0 <- 0.5
# 1) Simuliere x
x <- abs(rnorm(n, mean = 0, sd = 1))
# 2) Berechne für jede x_i die Fehler‐Std-Dev
sigma_x <- sigma0 * x
# 3) Ziehe heteroskedastische Fehler
eps <- rnorm(n, mean = 0, sd = sigma_x)
# 4) Erzeuge y
y <- beta * x + eps
# Kurzer Blick auf Varianz in Abhängigkeit von x
plot(x, y,
xlab = "x",
ylab = "y",
main = "Heteroskedastie")
dataframe <- as.data.frame(cbind(x, y))
View(dataframe)
colnames(dataframe) <- c("unab", "abh")
split <- train_val_test(dataframe["unab"])
train <- split$train
val <- split$val
test <- split$test
nrow(dataframe) == nrow(test)+nrow(train)+nrow(val)
train_loader <- DataLoader(split$train)
targets <- dataframe$abh
val_targets <- targets[as.integer(rownames(val))]
dimensions <- getLayerDimensions(train_loader[[1]]$batch, 2, hidden_neurons = 3)
train(train_loader, targets, dimensions, t(val), val_targets)
train(train_loader, targets, dimensions, t(val), val_targets)
load_all()
train(train_loader, targets, dimensions, t(val), val_targets)
#' @param beta2         Adam β2 (nur wenn optimizer = "adam", Default 0.999).
#' @param eps           Adam ε (nur wenn optimizer = "adam", Default 1e-8).
#' @param val_split  Optional: Matrix für Validierungs-Inputs.
#' @param val_targets  Optional: Vektor für Validierungs-Ziele.
#'
#' @return Liste mit
#' \item{params}{Gelernte Parameter}
#' \item{train_loss}{Vektor der Trainingsverluste pro Epoche}
#' \item{val_loss}{Optional: Vektor der Validierungsverluste pro Epoche}
#' @export
train <- function(
train_loader, targets, dimensions, val_split = NULL, val_targets = NULL,
epochs = 100, lr = 0.01,
optimizer = c("sgd", "adam"),
beta1 = 0.9, beta2 = 0.999, eps = 1e-8
) {
optimizer <- match.arg(optimizer)
params <- init_params(dimensions)
if (optimizer == "adam") {
opt <- list(
mW1 = matrix(0, dimensions$n_h, dimensions$n_x),
vW1 = matrix(0, dimensions$n_h, dimensions$n_x),
mb1 = matrix(0, dimensions$n_h, 1),
vb1 = matrix(0, dimensions$n_h, 1),
mW2 = matrix(0, dimensions$n_y, dimensions$n_h),
vW2 = matrix(0, dimensions$n_y, dimensions$n_h),
mb2 = matrix(0, dimensions$n_y, 1),
vb2 = matrix(0, dimensions$n_y, 1)
)
t_global <- 0
}
history_train <- numeric(epochs)
history_val   <- if (!is.null(val_split)) numeric(epochs) else NULL
# Trainingsloop
for (e in seq_len(epochs)) {
batch_losses <- numeric(length(train_loader))
for (i in seq_along(train_loader)) {
Xb  <- train_loader[[i]]$batch
yb  <- targets[train_loader[[i]]$idx]
fwd <- forward_onehidden(Xb, params)
# Loss
batch_losses[i] <- neg_log_lik(
yb, as.numeric(fwd$mu), as.numeric(fwd$log_sigma),
reduction = "mean"
) # mean oder raw?
# Gradienten
grads <- backprop_onehidden(Xb, yb, fwd, params)
if (optimizer == "sgd") {
# einfacher Gradientenschritt
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
} else {
# Adam-Update
t_global <- t_global + 1
# W1
tmp  <- update_adam(opt$mW1, opt$vW1, grads$dW1)
opt$mW1 <- tmp$m; opt$vW1 <- tmp$v
params$W1 <- params$W1 - tmp$delta
# b1
tmp  <- update_adam(opt$mb1, opt$vb1, grads$db1)
opt$mb1 <- tmp$m; opt$vb1 <- tmp$v
params$b1 <- params$b1 - tmp$delta
# W2
tmp  <- update_adam(opt$mW2, opt$vW2, grads$dW2)
opt$mW2 <- tmp$m; opt$vW2 <- tmp$v
params$W2 <- params$W2 - tmp$delta
# b2
tmp  <- update_adam(opt$mb2, opt$vb2, grads$db2)
opt$mb2 <- tmp$m; opt$vb2 <- tmp$v
params$b2 <- params$b2 - tmp$delta
}
}
# Loss‐Logging
history_train[e] <- mean(batch_losses) # wir berechnen mean of means
if (!is.null(val_split)) {
fwd_val <- forward_onehidden(val_split, params)
history_val[e] <- neg_log_lik(
val_targets,
as.numeric(fwd_val$mu),
as.numeric(fwd_val$log_sigma),
reduction = "mean"
)
message(sprintf(
"Epoch %3d/%d – Train: %.6f | Val: %.6f",
e, epochs, history_train[e], history_val[e]
))
} else {
message(sprintf(
"Epoch %3d/%d – Loss: %.6f",
e, epochs, history_train[e]
))
}
}
out <- list(
params = params,
train_loss = history_train
)
if (!is.null(history_val)) out$val_loss <- history_val
invisible(out)
}
train(train_loader, targets, dimensions, t(val), val_targets)
model <- train(train_loader, targets, dimensions, t(val), val_targets)
model
plot(length(model$train_loss), model$train_loss, type = "l", col = "blue", ylim = c(0, 1))
length(model$train_loss)
plot(length(model$train_loss), model$train_loss, type = "l", col = "blue")
plot(1:length(model$train_loss), model$train_loss, type = "l", col = "blue")
plot(1:length(model$val_loss), model$val_loss, type = "l", col = "red")))
plot(1:length(model$train_loss), model$train_loss, type = "l", col = "blue")
plot(1:length(model$val_loss), model$val_loss, type = "l", col = "red")
summary <- function(model, plot = TRUE) {
# Summary Plots
if (plot) {
epochs <- seq_along(model$train_loss)
if (!is.null(model$val_loss)) {
rng <- range(c(model$train_loss, model$val_loss))
} else {
rng <- range(model$train_loss)
}
# Summary Plot Training
plot(
epochs, model$train_loss, type = "l",
col  = "blue",
ylim = rng,
main = "Training vs. Validation Loss",
xlab = "Epoch",
ylab = "Loss"
)
if (!is.null(model$val_loss)) {
lines(epochs, model$val_loss, col = "red", lty = 2)
legend(
"topright",
legend = c("Train", "Validation"),
col    = c("blue", "red"),
lty    = c(1, 2),
bty    = "n"
)
}
}
invisible(NULL)
}
summary(model)
model
summary(model)
View(abdom)
abdom_split <- train_val_test(abdom['x'], normalization=TRUE)
abdom_targets <- abdom$y
class(abdom$y)
abdom$y
train_abdom <- abdom_split$train
val_abdom <- abdom_split$validation
abdom_split <- train_val_test(abdom['x'], normalization=TRUE)
abdom_targets <- abdom$y
train_abdom <- abdom_split$train
val_abdom <- abdom_split$validation
val_abdom_targets <- abdom_targets[as.integer(rownames(val_abdom_targets))]
val_abdom_targets <- abdom_targets[as.integer(rownames(val_abdom))]
model2 <- train(abdom_loader, abdom_loader, dimensions, t(val_abdom), val_abdom_targets)
abdom_loader <- DataLoader(train_abdom)
model2 <- train(abdom_loader, abdom_loader, dimensions, t(val_abdom), val_abdom_targets)
val_abdom
val_abdom_targets
abdom_loader
dimensions
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets)
model2
summary(model2)
model <- train(train_loader, targets, dimensions, t(val), val_targets, optimizer = "adam")
model <- train(train_loader, targets, dimensions, t(val), val_targets, optimizer = "adam")
params <- init_params(dimensions)
#' @param beta2         Adam β2 (nur wenn optimizer = "adam", Default 0.999).
#' @param eps           Adam ε (nur wenn optimizer = "adam", Default 1e-8).
#' @param val_split  Optional: Matrix für Validierungs-Inputs.
#' @param val_targets  Optional: Vektor für Validierungs-Ziele.
#'
#' @return Liste mit
#' \item{params}{Gelernte Parameter}
#' \item{train_loss}{Vektor der Trainingsverluste pro Epoche}
#' \item{val_loss}{Optional: Vektor der Validierungsverluste pro Epoche}
#' @export
train <- function(
train_loader, targets, dimensions, val_split = NULL, val_targets = NULL,
epochs = 100, lr = 0.01,
optimizer = c("sgd", "adam"),
beta1 = 0.9, beta2 = 0.999, eps = 1e-8
) {
optimizer <- match.arg(optimizer)
params <- init_params(dimensions)
if (optimizer == "adam") {
opt <- list(
mW1 = matrix(0, dimensions$n_h, dimensions$n_x),
vW1 = matrix(0, dimensions$n_h, dimensions$n_x),
mb1 = matrix(0, dimensions$n_h, 1),
vb1 = matrix(0, dimensions$n_h, 1),
mW2 = matrix(0, dimensions$n_y, dimensions$n_h),
vW2 = matrix(0, dimensions$n_y, dimensions$n_h),
mb2 = matrix(0, dimensions$n_y, 1),
vb2 = matrix(0, dimensions$n_y, 1)
)
t_global <- 0
}
history_train <- numeric(epochs)
history_val   <- if (!is.null(val_split)) numeric(epochs) else NULL
# Trainingsloop
for (e in seq_len(epochs)) {
batch_losses <- numeric(length(train_loader))
for (i in seq_along(train_loader)) {
Xb  <- train_loader[[i]]$batch
yb  <- targets[train_loader[[i]]$idx]
fwd <- forward_onehidden(Xb, params)
# Loss
batch_losses[i] <- neg_log_lik(
yb, as.numeric(fwd$mu), as.numeric(fwd$log_sigma),
reduction = "mean"
) # mean oder raw?
# Gradienten
grads <- backprop_onehidden(Xb, yb, fwd, params)
if (optimizer == "sgd") {
# einfacher Gradientenschritt
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
} else {
# Adam-Update
t_global <- t_global + 1
# W1
tmp  <- update_adam(opt$mW1, opt$vW1, grads$dW1, beta1, beta2, t_global, lr, eps)
opt$mW1 <- tmp$m; opt$vW1 <- tmp$v
params$W1 <- params$W1 - tmp$delta
# b1
tmp  <- update_adam(opt$mb1, opt$vb1, grads$db1, beta1, beta2, t_global, lr, eps)
opt$mb1 <- tmp$m; opt$vb1 <- tmp$v
params$b1 <- params$b1 - tmp$delta
# W2
tmp  <- update_adam(opt$mW2, opt$vW2, grads$dW2, beta1, beta2, t_global, lr, eps)
opt$mW2 <- tmp$m; opt$vW2 <- tmp$v
params$W2 <- params$W2 - tmp$delta
# b2
tmp  <- update_adam(opt$mb2, opt$vb2, grads$db2, beta1, beta2, t_global, lr, eps)
opt$mb2 <- tmp$m; opt$vb2 <- tmp$v
params$b2 <- params$b2 - tmp$delta
}
}
# Loss‐Logging
history_train[e] <- mean(batch_losses) # wir berechnen mean of means
if (!is.null(val_split)) {
fwd_val <- forward_onehidden(val_split, params)
history_val[e] <- neg_log_lik(
val_targets,
as.numeric(fwd_val$mu),
as.numeric(fwd_val$log_sigma),
reduction = "mean"
)
message(sprintf(
"Epoch %3d/%d – Train: %.6f | Val: %.6f",
e, epochs, history_train[e], history_val[e]
))
} else {
message(sprintf(
"Epoch %3d/%d – Loss: %.6f",
e, epochs, history_train[e]
))
}
}
out <- list(
params = params,
train_loss = history_train
)
if (!is.null(history_val)) out$val_loss <- history_val
invisible(out)
}
abdom_split <- train_val_test(abdom['x'], normalization=TRUE)
abdom_targets <- abdom$y
train_abdom <- abdom_split$train
val_abdom <- abdom_split$validation
val_abdom_targets <- abdom_targets[as.integer(rownames(val_abdom))]
abdom_loader <- DataLoader(train_abdom)
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="adam")
dimensions <- getLayerDimensions(train_loader[[1]]$batch, 2, hidden_neurons = 3)
dimensions <- getLayerDimensions(1, 2, hidden_neurons = 3)
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="adam")
dimensions <- getLayerDimensions(train_abdom[[1]]$batch, 2, hidden_neurons = 3)
dimensions <- getLayerDimensions(abdom_loader[[1]]$batch, 2, hidden_neurons = 3)
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="adam")
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="adam")
abdom_split <- train_val_test(abdom['x'], normalization=TRUE)
abdom_targets <- abdom$y
train_abdom <- abdom_split$train
val_abdom <- abdom_split$validation
val_abdom_targets <- abdom_targets[as.integer(rownames(val_abdom))]
abdom_loader <- DataLoader(train_abdom)
dimensions <- getLayerDimensions(abdom_loader[[1]]$batch, 2, hidden_neurons = 3)
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="adam")
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets, optimizer="sgd")
model2 <- train(abdom_loader, abdom_targets, dimensions, t(val_abdom), val_abdom_targets)
